{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c6069a-94f2-438a-b825-841df9171afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "curpus = \"\"\"The elephant is the world's largest living creature. The elephant is a species that is both intelligent and obedient.\n",
    "The elephant is a four-legged creature with two small eyes, two large ears, a trunk, and a short tail. The elephant's four legs are all thick and big.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bce8e9-5219-4a86-83fb-2e576d1b93a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ceab4b-c1b4-43f0-8b28-d9c6804ba534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elephant is the world's largest living creature. The elephant is a species that is both intelligent and obedient.\n",
      "The elephant is a four-legged creature with two small eyes, two large ears, a trunk, and a short tail. The elephant's four legs are all thick and big.\n"
     ]
    }
   ],
   "source": [
    "print(curpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecec0f56-82ba-4f3b-837a-6331dfd42600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\duggu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e03a4e-16e4-40a2-b849-e478d59a5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization ( sentence to paragraph )\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a957d0f3-d785-4fbf-8b09-40d1f09cd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = sent_tokenize(curpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c7d63a-7ec2-4822-8ab2-ad33e91230a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220dd1d6-97e1-4bbb-8b86-e3337540e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elephant is the world's largest living creature.\n",
      "The elephant is a species that is both intelligent and obedient.\n",
      "The elephant is a four-legged creature with two small eyes, two large ears, a trunk, and a short tail.\n",
      "The elephant's four legs are all thick and big.\n"
     ]
    }
   ],
   "source": [
    "for doc in document:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1a8f8a-23b8-46cf-901c-75d9e1f872bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'elephant',\n",
       " 'is',\n",
       " 'the',\n",
       " 'world',\n",
       " \"'s\",\n",
       " 'largest',\n",
       " 'living',\n",
       " 'creature',\n",
       " '.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " 'is',\n",
       " 'a',\n",
       " 'species',\n",
       " 'that',\n",
       " 'is',\n",
       " 'both',\n",
       " 'intelligent',\n",
       " 'and',\n",
       " 'obedient',\n",
       " '.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " 'is',\n",
       " 'a',\n",
       " 'four-legged',\n",
       " 'creature',\n",
       " 'with',\n",
       " 'two',\n",
       " 'small',\n",
       " 'eyes',\n",
       " ',',\n",
       " 'two',\n",
       " 'large',\n",
       " 'ears',\n",
       " ',',\n",
       " 'a',\n",
       " 'trunk',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'short',\n",
       " 'tail',\n",
       " '.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " \"'s\",\n",
       " 'four',\n",
       " 'legs',\n",
       " 'are',\n",
       " 'all',\n",
       " 'thick',\n",
       " 'and',\n",
       " 'big',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence into word \n",
    "word_tokenize(curpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf0566d-d09e-4fe5-8b84-7a937e00ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'elephant', 'is', 'the', 'world', \"'s\", 'largest', 'living', 'creature', '.']\n",
      "['The', 'elephant', 'is', 'a', 'species', 'that', 'is', 'both', 'intelligent', 'and', 'obedient', '.']\n",
      "['The', 'elephant', 'is', 'a', 'four-legged', 'creature', 'with', 'two', 'small', 'eyes', ',', 'two', 'large', 'ears', ',', 'a', 'trunk', ',', 'and', 'a', 'short', 'tail', '.']\n",
      "['The', 'elephant', \"'s\", 'four', 'legs', 'are', 'all', 'thick', 'and', 'big', '.']\n"
     ]
    }
   ],
   "source": [
    "for doc in document:\n",
    "    print(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c5158b3-c9b6-4073-958f-eafba70d0708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'elephant',\n",
       "  'is',\n",
       "  'the',\n",
       "  'world',\n",
       "  \"'s\",\n",
       "  'largest',\n",
       "  'living',\n",
       "  'creature',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'elephant',\n",
       "  'is',\n",
       "  'a',\n",
       "  'species',\n",
       "  'that',\n",
       "  'is',\n",
       "  'both',\n",
       "  'intelligent',\n",
       "  'and',\n",
       "  'obedient',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'elephant',\n",
       "  'is',\n",
       "  'a',\n",
       "  'four-legged',\n",
       "  'creature',\n",
       "  'with',\n",
       "  'two',\n",
       "  'small',\n",
       "  'eyes',\n",
       "  ',',\n",
       "  'two',\n",
       "  'large',\n",
       "  'ears',\n",
       "  ',',\n",
       "  'a',\n",
       "  'trunk',\n",
       "  ',',\n",
       "  'and',\n",
       "  'a',\n",
       "  'short',\n",
       "  'tail',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'elephant',\n",
       "  \"'s\",\n",
       "  'four',\n",
       "  'legs',\n",
       "  'are',\n",
       "  'all',\n",
       "  'thick',\n",
       "  'and',\n",
       "  'big',\n",
       "  '.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(t) for t in sent_tokenize(curpus)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a199351-0be7-496b-b0f0-e7a1ff8fca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 12), (13, 15), (16, 19), (20, 27), (28, 35), (36, 42), (43, 52)]\n",
      "[(0, 3), (4, 12), (13, 15), (16, 17), (18, 25), (26, 30), (31, 33), (34, 38), (39, 50), (51, 54), (55, 64)]\n",
      "[(0, 3), (4, 12), (13, 15), (16, 17), (18, 29), (30, 38), (39, 43), (44, 47), (48, 53), (54, 59), (60, 63), (64, 69), (70, 75), (76, 77), (78, 84), (85, 88), (89, 90), (91, 96), (97, 102)]\n",
      "[(0, 3), (4, 14), (15, 19), (20, 24), (25, 28), (29, 32), (33, 38), (39, 42), (43, 47)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "for doc in document:\n",
    "    print(list(WhitespaceTokenizer().span_tokenize(doc)))\n",
    "    #print(word_tokenize(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd62d98e-f616-439e-a2cc-aa42448589cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'elephant',\n",
       " 'is',\n",
       " 'the',\n",
       " 'world',\n",
       " \"'\",\n",
       " 's',\n",
       " 'largest',\n",
       " 'living',\n",
       " 'creature',\n",
       " '.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " 'is',\n",
       " 'a',\n",
       " 'species',\n",
       " 'that',\n",
       " 'is',\n",
       " 'both',\n",
       " 'intelligent',\n",
       " 'and',\n",
       " 'obedient',\n",
       " '.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " 'is',\n",
       " 'a',\n",
       " 'four',\n",
       " '-',\n",
       " 'legged',\n",
       " 'creature',\n",
       " 'with',\n",
       " 'two',\n",
       " 'small',\n",
       " 'eyes',\n",
       " ',',\n",
       " 'two',\n",
       " 'large',\n",
       " 'ears',\n",
       " ',',\n",
       " 'a',\n",
       " 'trunk',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'short',\n",
       " 'tail',\n",
       " '.',\n",
       " 'The',\n",
       " 'elephant',\n",
       " \"'\",\n",
       " 's',\n",
       " 'four',\n",
       " 'legs',\n",
       " 'are',\n",
       " 'all',\n",
       " 'thick',\n",
       " 'and',\n",
       " 'big',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(curpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "846c0cbd-cadf-4d4e-bbf7-ca590e168c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db66cfee-2954-4639-8251-3f57873bc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = RegexpStemmer('ing$|s$|e$|able$', min=4)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6ab1038-d94f-4786-8ba9-97b81589b227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e26f5d5-5158-414d-b2d3-4722a5b8adf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80f89e24-7d03-498f-a1bb-40725e188f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('comput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "496fde54-9704-4ed9-819e-44f92a3fe5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bc787f1-afb2-4462-9267-ebe770aa5d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('ingeating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09066873-c772-4091-94bf-329dc4dd9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = RegexpStemmer('ing|s$|e$|able$', min=4)  # able$ remove last char able & no $ -- remove from front and back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92d4aa2e-58ac-43f7-a2bc-9be7f4625c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c10d3d5e-531d-4d96-8d6c-0e62c0f4de01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dis'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.stem('disable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b444295-9b6a-4ba9-a5ce-f661f4453b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b8d9421-3747-498c-af8a-c25cb8896950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4664198-060a-4d16-ac90-69b588f0ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05e5cf57-3c73-4bba-9a54-a0d3f406cf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"Autobahnen\") # Stem a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad0877be-f8d0-402d-950a-b8b31c89e22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello i am fin'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"hello i am fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4e2e08c-ac94-4f6e-b5f7-da47d177266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e348c6-e8c9-44a1-9341-82829a8efc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
